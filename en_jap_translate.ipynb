{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "en_jap_translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schokoro/AttentionNMT/blob/dev/en_jap_translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn5ZropJzbay",
        "colab_type": "text"
      },
      "source": [
        "Первым делом проверим какая карточка нам досталась. Лучшим вариантом будет `Tesla P100-PCIE-16GB`, на остальных будет считаться медленнее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcr_Px46CRg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cpu':\n",
        "    raise RuntimeErrorx\n",
        "else:\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NIT1LbBOjdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install janome pandarallel  > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrVZOPD8upby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf AttentionNMT/\n",
        "!git clone -b dev https://github.com/schokoro/AttentionNMT.git > /dev/null\n",
        "# !pip install allennlp wget youtokentome ipymarkup seqeval livelossplot> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meHkHoZ4-_Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/AttentionNMT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZM_GyVL_Ifv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import AttentionNMT\n",
        "from AttentionNMT.transformer import Transformer, Encoder, Decoder\n",
        "from AttentionNMT.utils import count_parameters, initialize_weights, translate_sentence, evaluate_blue, train, evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUSkxY7m0Ica",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%autoreload 2\n",
        "from typing import Dict, List, Optional\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from tqdm.notebook import tqdm\n",
        "from pandarallel import pandarallel\n",
        "import torchtext\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import nltk\n",
        "import spacy\n",
        "import warnings\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from janome.tokenizer import Tokenizer as JTokenizer\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sns.set()\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings('ignore')\n",
        "pandarallel.initialize(progress_bar=False)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8DRhQOh0Icr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXL-4dSw4_Ue",
        "colab_type": "text"
      },
      "source": [
        "# Подготовка данных и EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNLtpHdGp2ZT",
        "colab_type": "text"
      },
      "source": [
        "Для обучения англо-японского переводчика будем использовать [Tanaka Corpus](http://www.edrdg.org/wiki/index.php/Tanaka_Corpus). Прямая ссылка на скачивание [здесь](ftp://ftp.monash.edu/pub/nihongo/examples.utf.gz).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDEnl0xEPT64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q ftp://ftp.monash.edu/pub/nihongo/examples.utf.gz \n",
        "!gunzip -f examples.utf.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr9FAV3gZ0TF",
        "colab_type": "text"
      },
      "source": [
        "## Подготовка корпуса\n",
        "\n",
        "Посмотрим, как выглядит корпус:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pScGAsyMwba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -4 examples.utf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfwPqCbRoL1",
        "colab_type": "text"
      },
      "source": [
        "Нам нужны только строки с префиксом `A:`\n",
        "Для токенизации английского языка будем использовать spacy, для японского - токенизатор janome. Для англо-японского перевода будет лучше, если японский текст мы развернём задом наперёд. Просто поверьте. Или можете проверить. Соответственно, при переводе нам будет необходимо развернуть переведённый текст обратно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFpngBkPPh4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "jt = JTokenizer()\n",
        "en_remove = re.compile(r'#ID=.+')\n",
        "is_token = re.compile(r'\\w+')\n",
        "\n",
        "\n",
        "def get_pair(line: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Принимает на вход строку из файла паралелльного корпуса, возвращает словарь\n",
        "     с японским и английским предложениями\n",
        "    \"\"\"\n",
        "    jap, en = line.strip().split('\\t')\n",
        "    jap = jap.replace('A: ', '').strip()\n",
        "    en = en_remove.sub('', en)\n",
        "    return {'en': en, 'jap': jap}\n",
        "  \n",
        "def src_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Токенизирует английский текст, возвращает список токенов.\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in nlp.tokenizer(text)]\n",
        "\n",
        "\n",
        "def trg_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Токенизирует японский текст, возвращает список токенов в обратном порядке\n",
        "    \"\"\"\n",
        "    return [token.base_form for token in jt.tokenize(text)][: : -1]  # if is_token.match(token.base_form)\n",
        "\n",
        "\n",
        "dicts = []\n",
        "with open('examples.utf', 'r') as fobj:\n",
        "    for i, line in enumerate(fobj):\n",
        "        if line.startswith('A:'):\n",
        "            dicts.append(get_pair(line))\n",
        "\n",
        "\n",
        "\n",
        "corpus_df = pd.DataFrame(dicts)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oRO9brQWF_e",
        "colab_type": "text"
      },
      "source": [
        "Посчитаем длины предложений в символах и посмотрим их корреляцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iZW08AbbmZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['en_len'] = corpus_df['en'].map(lambda x: len(x))\n",
        "corpus_df['jap_len'] = corpus_df['jap'].map(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYfx854VQ0eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df[['en_len','jap_len' ]].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qyNHUsvQ0Xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(\"en_len\", \"jap_len\", data=corpus_df, kind=\"reg\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfAL1esrWZxm",
        "colab_type": "text"
      },
      "source": [
        "А теперь - длины предложений в токенах и их корреляцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDnO9c6yQ0S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time corpus_df['en_tok_len'] = corpus_df['en'].parallel_apply(lambda x: len(src_tokenize(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWWfEP1nQ0Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time corpus_df['jap_tok_len'] = corpus_df['jap'].parallel_apply(lambda x: len(trg_tokenize(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5Meg4yU3c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df[['en_tok_len','jap_tok_len' ]].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DarnwNuQ0ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(\"en_tok_len\", \"jap_tok_len\", data=corpus_df, kind=\"reg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MMd06uPU3XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lsaMnn8Ws6p",
        "colab_type": "text"
      },
      "source": [
        "Выбросим из корпуса предложения длиннее чем `quantile(0.9)` и сохраним в файл."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7ahp2Jn6bO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df.en_tok_len.quantile(0.9), corpus_df.jap_tok_len.quantile(0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDrSJAB06bL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_corpus_df = corpus_df[\n",
        "    (corpus_df.en_tok_len < corpus_df.en_tok_len.quantile(0.9)) &\n",
        "    (corpus_df.jap_tok_len < corpus_df.jap_tok_len.quantile(0.9)) \n",
        "]\n",
        "corpus_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgx2QbKd6bIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_corpus_df.to_csv('en_jap_corpus.csv', index=None, columns=['en', 'jap'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbCgisWNGycw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_src = corpus_df.en_tok_len.max() + 2\n",
        "max_trg = corpus_df.jap_tok_len.max() + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7qljoS80Idb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(\n",
        "            tokenize = src_tokenize, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(\n",
        "            tokenize = trg_tokenize, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az0f3DKVHNMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "trn_data_fields = [(\"src\", SRC), (\"trg\", TRG)]\n",
        "\n",
        "dataset = torchtext.data.TabularDataset(\n",
        "    path='en_jap_corpus.csv',\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=trn_data_fields\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWZ4YbiYl5jT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = dataset.split(split_ratio=[0.98, 0.01, 0.01])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc4SUWsuHNBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data.src, min_freq=3)  # \n",
        "print(SRC.vocab.freqs.most_common(10))\n",
        "\n",
        "TRG.build_vocab(train_data.trg, min_freq=3)\n",
        "print(TRG.vocab.freqs.most_common(10))\n",
        "\n",
        "\n",
        "print(f'SRC.vocab: {len(SRC.vocab)}, TRG.vocab: {len(TRG.vocab)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op4pLJZg0IeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.src), # the BucketIterator needs to be told what function it should use to group the data.\n",
        "    sort_within_batch=False,\n",
        "    shuffle=True,\n",
        "    device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5m8g_our-SNR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTDEZXuD0Ifm",
        "colab_type": "text"
      },
      "source": [
        "## Training the Transformer Model\n",
        "\n",
        "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkgyZ6CN0Ifr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 128\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 4\n",
        "DEC_HEADS = 4\n",
        "ENC_PF_DIM = 2 * HID_DIM\n",
        "DEC_PF_DIM = 2 * HID_DIM\n",
        "ENC_DROPOUT = 0.07\n",
        "DEC_DROPOUT = 0.07\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              dropout=ENC_DROPOUT, \n",
        "              device=device,\n",
        "              max_length=max_src)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM,\n",
        "              dropout=DEC_DROPOUT, \n",
        "              device=device,\n",
        "              max_length=max_trg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgYnFmtQ0If1",
        "colab_type": "text"
      },
      "source": [
        "Then, use them to define our whole sequence-to-sequence encapsulating model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjSwbDq0If3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "try:\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    print('no model')\n",
        "\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
        "# model = Transformer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTWTpx8Q0If_",
        "colab_type": "text"
      },
      "source": [
        "We can check the number of parameters, noticing it is significantly less than the 37M for the convolutional sequence-to-sequence model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmyEc_PD0IgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXl6dhGH0IgL",
        "colab_type": "text"
      },
      "source": [
        "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtJJAvs0Igd",
        "colab_type": "text"
      },
      "source": [
        "The optimizer used in the original Transformer paper uses Adam with a learning rate that has a \"warm-up\" and then a \"cool-down\" period. BERT and other Transformer models use Adam with a fixed learning rate, so we will implement that. Check [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer) link for more details about the original Transformer's learning rate schedule.\n",
        "\n",
        "Note that the learning rate needs to be lower than the default used by Adam or else learning is unstable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM4dhePH0Ihf",
        "colab_type": "text"
      },
      "source": [
        "We then define a small function that we can use to tell us how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfxGr8IR0Ihi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRvv0V8bXE46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.apply(initialize_weights);\n",
        "best_macro_bleu = 0\n",
        "test_loss = np.inf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3XtxZn2AmHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEST_MODEL = 'best_blue_en_jap.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUPA8iz1yEDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross_entropy = CrossEntropyLoss(ignore_index = TRG_PAD_IDX, smooth_eps=0.03)\n",
        "cross_entropy = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "LEARNING_RATE = 1.25e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "def criterion(pred, target):\n",
        "    \"\"\"\n",
        "    pred - BatchSize x TargetLen x VocabSize\n",
        "    target - BatchSize x TargetLen\n",
        "    \"\"\"\n",
        "    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n",
        "    target_flat = target.view(-1)  # BatchSize*TargetLen\n",
        "    return cross_entropy(pred_flat, target_flat)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                       factor=0.2,\n",
        "                                                       verbose=True,\n",
        "                                                       cooldown=0, \n",
        "                                                       patience=4,              \n",
        "                                                       threshold=0.01,\n",
        "                                                       min_lr=5e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZBBL0ORs7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train(model, train_iterator, optimizer, criterion, CLIP, 128)\n",
        "train_history = []\n",
        "valid_history = []\n",
        "blue_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hlw1p_fD-QcW",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 40\n",
        "CLIP = 1\n",
        "\n",
        "try:\n",
        "    best_valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "except:\n",
        "    best_valid_loss = float('inf')\n",
        "print(f'Loss: {best_valid_loss:7.3f}')\n",
        "\n",
        "\n",
        "for epoc_num, epoch in tqdm(enumerate(range(N_EPOCHS)), total=N_EPOCHS):\n",
        "    curent_lr = optimizer.param_groups[0]['lr']\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP, 1, 1)\n",
        "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "        scheduler.step(valid_loss)\n",
        "                \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            print('Новая лучшая модель')\n",
        "\n",
        "        macro_bleu = evaluate_blue(valid_data, SRC, TRG, model, device)\n",
        "        if macro_bleu > best_macro_bleu:\n",
        "            best_macro_bleu = macro_bleu\n",
        "            print('Новый лучший blue')            \n",
        "            torch.save(model.state_dict(), BEST_MODEL)\n",
        "\n",
        "        train_history.append(train_loss)\n",
        "        valid_history.append(valid_loss)\n",
        "        blue_history.append(macro_bleu)\n",
        "\n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        \n",
        "        print(f'\\tMacro-average BLEU: {macro_bleu:.6f}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        print('_' * 100)\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Epoch: {epoch+1:02} | LR: {optimizer.param_groups[0]['lr']}\")\n",
        "        break\n",
        "    finally:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaLO2yNdGem2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6yyyi4f0Ih5",
        "colab_type": "text"
      },
      "source": [
        "We load our \"best\" parameters and manage to achieve a better test perplexity than all previous models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WsXCafR0Ih7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load(BEST_MODEL))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IdR6k9gv824w"
      },
      "source": [
        "We'll now define a function that displays the attention over the source sentence for each step of the decoding. As this model has 8 heads our model we can view the attention for each of the heads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-rQGvLM824n"
      },
      "source": [
        "First, we'll get an example from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx1zXO1Y5SbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okKsDx0q8ZJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://github.com/vuthaihoc/fonts/raw/master/JUSTRAJDEEP/Osaka.ttf\n",
        "\n",
        "fontprop_x = FontProperties(size=20)\n",
        "fontprop_y = FontProperties(fname='Osaka.ttf', size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7o6un_Yy9FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def display_attention(sentence:List[str], translation, attention, n_heads = DEC_HEADS, n_rows = DEC_HEADS/2, n_cols = 2):\n",
        "    \n",
        "#     assert n_rows * n_cols == n_heads\n",
        "\n",
        "#     translation = translation[::-1]\n",
        "#     attention = torch.flip(attention, (2,))\n",
        "    \n",
        "#     fig = plt.figure(figsize=((n_cols + 1.5) * 6,(n_rows + 1) * 6))\n",
        "    \n",
        "#     for i in range(n_heads):\n",
        "        \n",
        "#         ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "#         _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "#         cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "#         ax.tick_params(labelsize=20)\n",
        "#         ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'],  #\n",
        "#                            rotation=45, fontproperties=fontprop_x)\n",
        "#         ax.set_yticklabels(['']+translation, fontproperties=fontprop_y)\n",
        "\n",
        "#         ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "#         ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "#     plt.show()\n",
        "#     plt.close()\n",
        "\n",
        "# def sentence_blue(original: List[str], translation: List[str], n_grams=3) -> float:\n",
        "#     blue = nltk.translate.bleu_score.sentence_bleu(\n",
        "#         original,\n",
        "#         translation[: -1],\n",
        "#         weights = [1/n_grams] * n_grams\n",
        "#         ) \n",
        "#     return blue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QMTdkSK9824g",
        "colab": {}
      },
      "source": [
        "example_idx = random.randint(0, len(train_data) - 1)\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['src']\n",
        "trg = vars(train_data.examples[example_idx])['trg']\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'src      : {\" \".join(src)}')\n",
        "print(f'trg      : {\" \".join(trg[::-1])}')\n",
        "print(f'predicted: {\" \".join(translation[::-1])}')\n",
        "print(f'     blue: {100 * sentence_blue(trg, translation):.2f}%')\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nRsOvhp824N",
        "colab": {}
      },
      "source": [
        "example_idx = random.randint(0, len(valid_data) - 1)\n",
        "\n",
        "src = vars(valid_data.examples[example_idx])['src']\n",
        "trg = vars(valid_data.examples[example_idx])['trg']\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'src      : {\" \".join(src)}')\n",
        "print(f'trg      : {\" \".join(trg[::-1])}')\n",
        "print(f'predicted: {\" \".join(translation[::-1])}')\n",
        "print(f'     blue: {100 * sentence_blue(trg, translation):.2f}%')\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rMvJiQZl823a",
        "colab": {}
      },
      "source": [
        "example_idx = random.randint(0, len(test_data) - 1)\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['src']\n",
        "trg = vars(test_data.examples[example_idx])['trg']\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'src      : {\" \".join(src)}')\n",
        "print(f'trg      : {\" \".join(trg[::-1])}')\n",
        "print(f'predicted: {\" \".join(translation[::-1])}')\n",
        "print(f'     blue: {100 * sentence_blue(trg, translation):.2f}%')\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O4dwWnT5zv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idxs = list(range(len(test_data)))\n",
        "random.shuffle(idxs)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for example_idx in idxs:\n",
        "    example_idx = random.randint(0, 1219)\n",
        "    src = vars(test_data.examples[example_idx])['src']\n",
        "    trg = vars(test_data.examples[example_idx])['trg']\n",
        "    translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "    result = [translation[0]]\n",
        "    \n",
        "    blue = sentence_blue(trg, translation, 4)\n",
        "    if blue < .85:\n",
        "        continue\n",
        "\n",
        "\n",
        "    for token in translation:\n",
        "        if token != result[-1]:\n",
        "            result.append(token)\n",
        "    print(f'src      : {\" \".join(src)}')\n",
        "    print(f'trg      : {\" \".join(trg[::-1])}')\n",
        "    print(f'predicted: {\" \".join(translation[::-1])}')\n",
        "    print(f'     blue: {100 * blue:.2f}%')\n",
        "    display_attention(src, translation, attention)\n",
        "\n",
        "    count += 1\n",
        "    if count > 40:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNScDLRTRyig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}